---
title: "Scraping RUG Sociology department"
author:
- Rob Franken
- Daniel Cowen
- Jochem Tolsma
- '**AUTHORS:**'
date: "Last compiled on `r format(Sys.time(), '%B, %Y')`"
output:
  html_document:
    css: tweaks.css
    toc: true
    toc_float: true
    number_sections: false
    toc_depth: 2
    code_folding: show
    code_download: true
  word_document:
    toc: true
    toc_depth: '2'
  pdf_document:
    toc: true
    toc_depth: '2'
link-citations: true
---


```{r, globalsettings, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(knitr)
library(tidyverse) 
library(reticulate)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=100),tidy=TRUE, warning = FALSE, message = FALSE,comment = "#>", cache=TRUE, class.source=c("test"), class.output=c("test3"))
options(width = 100)
rgl::setupKnitr()

colorize <- function(x, color) {sprintf("<span style='color: %s;'>%s</span>", color, x) }
```

```{r klippy, echo=FALSE, include=TRUE}
# install.packages("remotes",repos = "http://cran.us.r-project.org")
# remotes::install_github("rlesur/klippy")
# klippy::klippy(lang = c("r", "python"), position = c('top', 'right'))
# 
# klippy::klippy(color = 'darkred')
# klippy::klippy(tooltip_message = 'Click to copy', tooltip_success = 'Done')
```

# Preparation

## clean up
```{r, cleanup, results='hide'}
rm(list=ls())
gc()
```

<br>

## general custom R functions

- `fpackage.check`: Check if packages are installed (and install if not) in R
- `fsave`: save data with time stamp in correct directory
- `fload`: load R-objects under new names
- `fshowdf`: Print objects (`tibble` / `data.frame`) nicely on screen in `.Rmd`.


```{r, customfunc}
fpackage.check <- function(packages) {
    lapply(packages, FUN = function(x) {
        if (!require(x, character.only = TRUE)) {
            install.packages(x, dependencies = TRUE)
            library(x, character.only = TRUE)
        }
    })
}

fsave <- function(x, file, location = "./local/", ...) {
    if (!dir.exists(location))
        dir.create(location)
    datename <- substr(gsub("[:-]", "", Sys.time()), 1, 8)
    totalname <- paste(location, datename, file, sep = "")
    print(paste("SAVED: ", totalname, sep = ""))
    save(x, file = totalname)
}

fload  <- function(fileName){
  load(fileName)
  get(ls()[ls() != "fileName"])
}


fshowdf <- function(x, digits = 2, ...) {
    knitr::kable(x, digits = digits, "html", ...) %>%
        kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
        kableExtra::scroll_box(width = "100%", height = "300px")
}
```

<br>

## necessary packages {.tabset .tabset-fade}

- `rvest`: part of tidyverse, not necessary to load seperately

### R

```{r, packages, results='hide', message=FALSE, warning=FALSE}
packages = c( "rvest", "tidyverse", "stringr","xml2")
fpackage.check(packages)
rm(packages)
```

### Python

Make sure you have the required libraries by typing in the code below into your console
```{bash, eval = FALSE}
pip install requests beautifulsoup4 pandas
``` 

<br>

and import necessary modules

```{python, eval = FALSE}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
``` 

<br>

## Input the static URL you wish to scrape {.tabset .tabset-fade}

### R

<!-- ```{r} -->

<!-- library(chromote) -->

<!-- b <- ChromoteSession$new() -->


<!-- b$view() -->
<!-- b$Page$navigate(url) -->
<!-- sess <- LiveHTML$new(url) -->
<!-- sess$view() -->
<!-- sess$html_elements(".rug-ph-xs")  -->
<!-- ``` -->


```{r, rvest, eval = FALSE}
url <- "https://www.rug.nl/about-ug/practical-matters/find-an-expert?lang=en&discipline=Sociology"


RugSociology <- read_html(url)
``` 
<br>


## {.unlisted .unnumbered}


<br>

---

# Scraping scholar names {.tabset .tabset-fade}

## R

You will need to know the tags or XPath to the exact types of elements you are looking for. You can do this by inspecting the webpage itself and finding the common code between all items you want to get.

Alternatively, you can use extensions such as Selector Gadget to get the XPath or tags you are looking for. Here, I am inputting the code used to specify information from an individual staff member

```{r, names, eval = FALSE, results = 'asis'}


Staffmembers <- RugSociology %>% html_elements(".rug-ph-xs") 
#and subsequently verify that the first entry into my staff members list is indeed the first person on the webpage
xml_child(Staffmembers[[1]], 1)
xml_child(Staffmembers[[2]], 1)

#Again, then using Selector Gadget or looking at the similarities between each individual staff members list, we can find the tag that corresponds just to the names, and extract just the text from this using html_text2

StaffNames <- Staffmembers %>% html_nodes("div") %>% html_elements(".rug-h3") %>% html_text2()
#for the difference between html_text/2: https://rvest.tidyverse.org/reference/html_text.html


head(StaffNames)



#similar can be done for other information like their email

#JT, how did you find this element. Explain naming. 
StaffEmail <- Staffmembers %>% html_element(".rug-mb-s:nth-child(1) .rug-width-s-20-24") %>% html_text2()
#necessary to clean text
StaffEmail<- gsub("[\r\n0-9+/]", "", StaffEmail)
#remove all the trailing spaces after cleaning (using the (.nl).*) while still keeping the end of the email (using \\1)
StaffEmail <- gsub("(.nl).*", "\\1", StaffEmail)
#Finally, we see we didnt scrape the @ (as it is a symbol that also means something else in html so the html_text function didnt think it was text and subsequently removed it) so we must put it back in place of the space it left
StaffEmail <- gsub(" ", "@", StaffEmail)
#and then we have all 118 emails!
head(StaffEmail)

#Fields of Research
#Similar to above, separate teh text from html code and clean as needed
StaffExpertese <- Staffmembers %>% html_element(".rug-mb-s~ .rug-mb-s+ .rug-mb-s .rug-width-s-20-24") %>% html_text2()
#Here we remove line breaks and replace them with semi-colons 
StaffExpertese<- gsub("[\r\n]", "; ", StaffExpertese)
head(StaffExpertese)

#Function/role
#Similar to above, separate the text from html code and clean as needed. This time no cleaning is needed
StaffRole <- Staffmembers %>% html_element(".rug-layout:nth-child(2) .rug-width-s-20-24") %>% html_text2()
head(StaffRole)


#Misc Info
#Similar to above, separate teh text from html code and clean as needed. This time no cleaning is needed
StaffInfo <- Staffmembers %>% html_element(".rug-h3+ .rug-mb-s") %>% html_text2()
head(StaffInfo)


#After we get all the information we want, we can then combine it into a single dataframe to allow for better storage and data manipulation

RugSociologyEmployees <- data.frame(cbind(StaffNames,StaffEmail,StaffRole,StaffExpertese,StaffInfo))

fshowdf(RugSociologyEmployees, caption = "Our table of all the Sociology Employees in the RuG" ) 


``` 

## Python

```{python, eval = FALSE}
# Fetch the webpage
url = "https://www.rug.nl/about-ug/practical-matters/find-an-expert?lang=en&discipline=Sociology"
response = requests.get(url)
webpage = response.content

# Parse the webpage
soup = BeautifulSoup(webpage, 'html.parser')

# Extract staff members
staff_members = soup.select('.rug-ph-xs')

# Verify the first entry
first_staff_member = staff_members[0]
second_staff_member = staff_members[1]
print(first_staff_member.prettify())
print(second_staff_member.prettify())

# Extract names
staff_names = [member.select_one('.rug-h3').get_text(strip=True) for member in staff_members]
print(staff_names[:6])

# Extract emails
staff_emails = [member.select_one('.rug-mb-s:nth-child(1) .rug-width-s-20-24').get_text(strip=True) for member in staff_members]
# Clean emails
staff_emails = [re.sub(r'[\r\n0-9+/]', '', email) for email in staff_emails]
staff_emails = [re.sub(r'(.nl).*', r'\1', email) for email in staff_emails]
staff_emails = [email.replace(' ', '@') for email in staff_emails]
print(staff_emails[:6])

# Extract fields of research and do the same cleaning (with the different characters we notice are in these fields)
staff_expertise = [member.select_one('.rug-mb-s~ .rug-mb-s+ .rug-mb-s .rug-width-s-20-24').get_text(strip=True) for member in staff_members]
staff_expertise = [expertise.replace('\r\n', '; ') for expertise in staff_expertise]
print(staff_expertise[:6])

# Extract roles
staff_roles = [member.select_one('.rug-layout:nth-child(2) .rug-width-s-20-24').get_text(strip=True) for member in staff_members]
print(staff_roles[:6])

# Extract miscellaneous info
staff_info = [member.select_one('.rug-h3+ .rug-mb-s').get_text(strip=True) for member in staff_members]
print(staff_info[:6])

# Combine data into a DataFrame
rug_sociology_employees = pd.DataFrame({
    'Name': staff_names,
    'Email': staff_emails,
    'Role': staff_roles,
    'Expertise': staff_expertise,
    'Info': staff_info
})

# Display the DataFrame
print(rug_sociology_employees.head())

# Optionally save to a file
#rug_sociology_employees.to_csv("rug_sociology_employees.csv", index=False)
``` 

