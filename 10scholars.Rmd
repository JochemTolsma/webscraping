---
title: "Webscraping scholars"
bibliography: references.bib
link-citations: yes
---


```{r, globalsettings, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=100),tidy=TRUE, warning = FALSE, message = FALSE,comment = "#>", cache=TRUE, class.source=c("test"), class.output=c("test2"), cache.lazy = FALSE)
options(width = 100) 
rgl::setupKnitr()

colorize <- function(x, color) {sprintf("<span style='color: %s;'>%s</span>", color, x) }

```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE}
# install.packages("remotes")
#remotes::install_github("rlesur/klippy")
klippy::klippy(position = c('top', 'right'))
#klippy::klippy(color = 'darkred')
#klippy::klippy(tooltip_message = 'Click to copy', tooltip_success = 'Done')
```

# Goal  

**Hands-on webscraping**  

It is time to do some webscraping ourselves. In what follows is a short first tutorial on webscraping where we will be collecting data from webpages on the internet. We will use the specific use case of the political science department staff of the university of Leiden. 

What do they publish? Where? And with whom do they collaborate? We assume you have at least some experience with coding in R. In the rest of this part of the tutorial, we will switch between base R and Tidyverse (just a bit), whatever is most convenient. (Note that this will happen often if you become an applied computational sociologist.) We also offer Python code. 

There are different strategies in scraping. There is often a trade-off between complex scraping techniques versus complex string manipulations. In this first example, we will use quite a lot of string manipulations. 

For even more info see our [SNASS book - Chapter 11](https://snass.netlify.app/webintro.html)  

---  

# Preparation

## clean up
```{r, cleanup, results='hide'}
rm(list=ls())
gc()
```

## Custom functions

- `fpackage.check`: Check if packages are installed (and install if not) in R ([source](https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/)).  
- `fsave`: Save to processed data in repository  
- `fload`: To load the files back after an `fsave`  
- `fshowdf`: To print objects (tibbles / data.frame) nicely on screen in .rmd  


```{r customfunctions, results='hide'}
rm(list = ls()) #clean up your environment

fpackage.check <- function(packages) {
  lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  })
}

fsave <- function(x, file=NULL, location="./data/processed/") {
  ifelse(!dir.exists("data"), dir.create("data"), FALSE)
  ifelse(!dir.exists("data/processed"), dir.create("data/processed"), FALSE)
  if (is.null(file)) file= deparse(substitute(x))
  datename <- substr(gsub("[:-]", "", Sys.time()), 1,8)  
  totalname <- paste(location, datename, file, ".rda", sep="")
  save(x, file = totalname)  #need to fix if file is reloaded as input name, not as x. 
}

fload <- function(filename) {
  load(filename)
  get(ls()[ls() != "filename"])
}

fshowdf <-  function(x, ...) {
  knitr::kable(x, digits=2, "html", ...) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width="100%", height= "300px")
} 

```

----

## Packages {.tabset .tabset-fade}

### R 

- `tidyverse`: for piping etc. 
- `httr`: Tools for Working with URLs and HTTP  
- `xml2`: Work with XML files using a simple, consistent interface.  
- `rvest`: Wrappers around the 'xml2' and 'httr' packages to make it easy to download, then manipulate, HTML and XML.  
- `reshape2`: Flexibly Reshape Data  

```{r, results='hide'}
packages = c("tidyverse", "httr", "rvest", "reshape2", "xml2")
fpackage.check(packages)
```

---  

### Python

Make sure you have the required libraries by typing in the code below into your console

```{bash, eval = FALSE}
pip install requests beautifulsoup4 pandas
``` 

<br>

and import necessary modules

```{python, eval = FALSE}

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

``` 

<br>


# Leiden University: Political Science

What do we mean by anchor data? Our goal is to get to know:  

i. who the Political Science staff is at several universities,  
ii. what they publish with respect to scientific work, and  
iii. who they collaborate with.  

So that means at least three data sources we need to collect from somewhere. What would be a nice starting (read: anchor) point be? First, we have to know who is staff. Let's check out [the Leiden political science staff website](https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1). Here we see a nice list on who is on the staff in several pages. How do we get that data? It is actually quite simple, the package [`rvest`](https://rvest.tidyverse.org/index.html) has a very nice function [`html_read()`](https://rvest.tidyverse.org/reference/read_html.html) (actually this comes from the `xml2` package) which simply derives the source html of a *static* webpage:

## Staff pages {.tabset .tabset-fade}

### R 

```{r}
# Let's first simply get the staff pages
# read_html is a function that simply extracts html webpages and puts them in xml format

lpol_staff <- read_html("https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1")
head(lpol_staff)
```
That looks kinda weird. What type of object did we store it by putting the html into `lpol_staff1`?

```{r}
class(lpol_staff)
```

So it is is stored in something that's called an xml object. Not important for now what that is. But it is important to extract the relevant table that we saw on the staff website. How do we do that? Go to one of the links above in a browser and then press "Inspect" on the webpage (usually: right click--\>Inspect). In the html code we extracted, we need to go to one of the nodes first. If you move your cursor over **"div"** in the html code on the screen, the entire **"body"** of the page should become some shade of blue. This means that the elements encapsulated in the **"body"** node captures everything that turned blue.


### Python

```{python, eval = FALSE}
# Fetch the webpage
url = "https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1"

response = requests.get(url)
webpage = response.content

# Parse the webpage
soup = BeautifulSoup(webpage, 'html.parser')

#print(soup.prettify())

#or to print just a few lines
result = soup.prettify().splitlines()
print('\n'.join(result[:10]))
```
```
<!DOCTYPE html>
<html data-version="1.178.00" lang="en">
 <head>
  <!-- standard page html head -->
  <title>
   Staff - Leiden University
  </title>
  <meta content="o8KYuFAiSZi6QWW1wxqKFvT1WQwN-BxruU42si9YjXw" name="google-site-verification"/>
  <meta content="hRUxrqIARMinLW2dRXrPpmtLtymnOTsg0Pl3WjHWQ4w" name="google-site-verification"/>
  <link href="https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff" rel="canonical"/>
```


## Nodes and tags {.tabset .tabset-fade}


Next, we need to look at the specific elements on the page that we need to extract. Somewhat by informed trial and error, looking for the correct code, we can select the elements we want. So we need code that looks for the node **"main"** and the **"td"** elements in the xml object and then extract those elements in it. Note that you can click on the arrows once you are in the "Inspect" mode in the web browser to trial-and-error to get at the correct elements.  

There are many ways to find the CSS or Xpath of the elements you want. 
The package `rvest` also has a SelectorGadget tool. See [here](https://rvest.tidyverse.org/articles/selectorgadget.html)

Wo we need to find WHERE the table is located in the html. 

Assignment:  
- use firefox 'inspect'  
- use rstudio  
- use `rvest` SelectorGadget

### R 

```{r}
lpol_staff <- lpol_staff %>% 
  html_nodes("body") %>%
  html_nodes(xpath = "//a") %>% 
  html_text()
```

Let us for now assume you are satisfied with the above output.

## Python
```{python, eval = FALSE}
lpol_staff = soup.body.find_all('a')

lpol_staff = [tags.prettify() for tags in lpol_staff]
print(lpol_staff[:10])
```
```
['<a class="active track-event" data-event-category="external-site" data-event-label="Topmenu external-site" href="/en">\n Leiden University\n</a>\n', '<a class="track-event" data-event-category="student-site" data-event-label="Topmenu external-site" href="https://www.student.universiteitleiden.nl/en">\n Students\n</a>\n', '<a class="track-event" data-event-category="staffmember-site" data-event-label="Topmenu external-site" href="https://www.staff.universiteitleiden.nl/">\n Staff members\n</a>\n', '<a class="track-event" data-event-category="org-site" data-event-label="Topmenu external-site" href="https://www.organisatiegids.universiteitleiden.nl/en">\n Organisational structure\n</a>\n', '<a class="track-event" data-event-category="library-site" data-event-label="Topmenu external-site" href="https://www.library.universiteitleiden.nl/">\n Library\n</a>\n', '<a href="/en">\n <img alt="Universiteit Leiden" height="64" src="/en/design-1.0/assets/images/zegel.png" width="151"/>\n</a>\n', '<a data-hidden="" data-hint="Search for subject or person" href="/en/search">\n All categories\n</a>\n', '<a data-hidden=\'{"content-category":"staffmember"}\' data-hint="Search for\xa0persons" href="/en/search">\n Persons\n</a>\n', '<a data-hidden=\'{"content-category":"education"}\' data-hint="Search for\xa0education" href="/en/search">\n Education\n</a>\n', '<a data-hidden=\'{"content-category":"research"}\' data-hint="Search for\xa0research" href="/en/search">\n Research\n</a>\n']
```

## Cleaning 

### R 

Seems like more useful data now. But can we improve by deleting some elements we do not need? Let's first delete some of the useless information.

```{r}
head(lpol_staff)
lpol_staff <-lpol_staff[-c(1:39)]
lpol_staff<- lpol_staff[-c(133:length(lpol_staff))]
head(lpol_staff)

fshowdf(lpol_staff)
```

Still looks a bit messy. Can we get it into a dataframe and split the column into useful columns?

```{r}
lpol_staff <- data.frame(lpol_staff)
lpol_staff <- colsplit(lpol_staff$lpol_staff, "          ", names = c("v1","v2","v3","v4","v5"))
fshowdf(lpol_staff, caption="lpol_staff")
```

Nice! I think we only need column 4 and 5? And let's name them nicely and delete any trailing or leading whitespace.

```{r}
lpol_staff <- lpol_staff[, c("v3","v4")]
names(lpol_staff) <- c("name", "func")

lpol_staff$name <- trimws(lpol_staff$name, which = c("both"), whitespace = "[ \t\r\n]")
lpol_staff$func <- trimws(lpol_staff$func, which = c("both"), whitespace = "[ \t\r\n]")
fshowdf(lpol_staff, caption="lpol_staff")
```

Not bad. 

## Love Scraping? {.tabset .tabset-fade}

Suppose you do not like datawrangling but simply love scraping. 

### R

```{r}
lpol_staff_names <- read_html("https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1") %>%
  html_element("section.tab.active") %>%
  html_elements("ul.table-list") %>%
  html_elements("li") %>% 
  html_elements("a") %>% 
  html_elements("div") %>%
  html_elements("strong")%>%
  html_text()

lpol_staff_functions <- read_html("https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1") %>%
  html_element("section.tab.active") %>%
  html_elements("ul.table-list") %>%
  html_elements("li") %>% 
  html_elements("a") %>% 
  html_elements("div") %>%
  html_elements("span")%>%
  html_text()
  
  lpol_staff2 <- data.frame(name = lpol_staff_names, funct = lpol_staff_functions)
  fshowdf(lpol_staff2)

```
### Python

```{python, eval = FALSE}
url = "https://www.universiteitleiden.nl/en/social-behavioural-sciences/political-science/staff#tab-1"
response = requests.get(url)
webpage = response.content

# Parse the webpage
soup = BeautifulSoup(webpage, 'html.parser')

#save the tags 'li' in a list
soup2 = soup.select_one('section.tab.active').select_one('ul.table-list').find_all("li")

#loop through the list to find what we want
lpol_staff_names = [taga.select_one("strong").get_text() for taga in soup2]
lpol_staff_functions = [taga.select_one("span").get_text() for taga in soup2]

# Combine data into a DataFrame
lpol_staff_df = pd.DataFrame({
    'name': lpol_staff_names,
    'funct': lpol_staff_functions
})

# Display the DataFrame
print(lpol_staff_df.head())  

```
```
                name                                            funct
0        Adina Akbik                       Senior Assistant Professor
1       Femke Bakker                       Senior assistant professor
2  Ingrid van Biezen                Professor of Comparative Politics
3     Nicolas Blarel                              Associate Professor
4         Arjen Boin  Professor of Public Institutions and Governance
```


